\documentclass{beamer}
%Information to be included in the title page:
\title{Optimization notes}
\author{Ying-Jia Lin}
\institute{National Cheng Kung University}

\date{May 3rd, 2021}
\begin{document}  
\frame{\titlepage}            
\begin{frame}
    \frametitle{Directional derivative}
    From a starting point $\underline{x}_0$ and a given dirction $\underline{u}$:
    \setbeamertemplate{itemize items}[circle]
    \begin{itemize}
        \item $\underline{x}(\lambda)=\underline{x}_0+\lambda \underline{u}$
        \begin{itemize}
            \item $\lambda$ is a scalar.
        \end{itemize}
        \item \alert{$d\underline{x}=\underline{u}d\lambda$}
        \begin{itemize}
            \item For a small change in $\lambda$.
        \end{itemize}
        \item $F(\lambda)=f(\underline{x}_0+\lambda\underline{u})$
        \begin{flalign*}
            &\begin{aligned}
            dF=df&=(\triangledown f(\underline{x}))^\top d\underline{x}\\
                 &=(\triangledown f(\underline{x}))^\top \alert{\underline{u}d \lambda}
                 =\triangledown ^\top f\underline{u}\lambda
            \end{aligned}&&
        \end{flalign*}
        \item $\frac{df}{d\lambda}=\triangledown ^\top f\underline{u}$
        \begin{itemize}
            \item If $f$ is minimized at $\underline{x}^*=\underline{x}_0+\lambda\underline{u}$, then:
            \begin{itemize}
                \item $\triangledown f(\underline{x}^*))^\top f\underline{u}=0$
                \item gradient $f$ evaluated at the minimum point is orthogonalto $\underline{u}$.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}
\begin{frame}
    \frametitle{Weierstrass Theorem}
    If $f(\underline{x})$ is continuous on a nonempty feasible set that is cloased and bounded,
    then $f(\underline{x})$ has a global minimum in this set.
    \begin{itemize}
        \item A set $S$ is bounded if for any point $\underline{x}$ in $S$, we have $\underline{x}^\top \underline{x}<c$
        \begin{itemize}
            \item $c$ is a finite positive number.
        \end{itemize}
    \end{itemize}
    
\end{frame}
\begin{frame}
    \frametitle{Single-variable unconstrained optimization}
    \setbeamertemplate{itemize items}[circle]
    \begin{itemize}
        \item Necessary condition
        \begin{itemize}
            \item If a function $f(x)$ has a local minimum at $x=x^*$,
            and $f'(x)$ exists as a finite number at $x=x^*$, then $f'(x^*)=0.$
            \item $x^*$ at $f'(x^*)=0$ is called stationary point.
        \end{itemize}
        \item Sufficient condition
        \begin{itemize}
            \item Suppose $f'(x^*)=f''(x^*)=\dots =f^{(m-1)}(x^*)=0$,
            but $f^{(m)}(x^*)\neq 0$, then $f(x^*)$ is:
            \begin{itemize}
                \item 1. a local minimum if $f^{(m-1)}(x^*)>0$ and $m$ is even.
                \item 2. a local maximum if $f^{(m-1)}(x^*)<0$ and $m$ is even.
                \item 3. neither a maximum nor a minimum if $m$ is odd.
            \end{itemize}
        \end{itemize}     
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Multi-variable unconstrained optimization (1)}
    Definition of $r^{th}$ differential of function $f$:
    \begin{flalign*}
        &\begin{aligned}
            d^rf(\underline{x}^*)=\sum^n_{i=1}\sum^n_{j=1}\dots\sum^n_{k=1}h_ih_j\dots h_k\frac{\partial^r{f(\underline{x}^*)}}{\partial{x_i}\partial{x_j}\dots\partial{x_k}}
        \end{aligned}&&
    \end{flalign*}
    \textbf{Example}
    
    When (order) $r=2$ and (number of variables) $n=3$, we have:

    \begin{flalign*}
        &\begin{aligned}
            d^2f(\underline{x}^*)&=d^2f(x_1^*, x_2^*, x_3^*)=\sum^3_{i=1}\sum^3_{j=1}h_ih_j\frac{\partial^2f(\underline{x}^*)}{\partial x_i\partial x_j}\\
            &=h^2_1\frac{\partial^2f(\underline{x}^*)}{\partial x^2_1}+
            h^2_2\frac{\partial^2f(\underline{x}^*)}{\partial x^2_2}+
            h^2_3\frac{\partial^2f(\underline{x}^*)}{\partial x^2_3} \\
            &+2h_1h_2\frac{\partial^2f(\underline{x}^*)}{\partial x_1\partial x_2}+
            2h_2h_3\frac{\partial^2f(\underline{x}^*)}{\partial x_2\partial x_3}+
            2h_1h_3\frac{\partial^2f(\underline{x}^*)}{\partial x_1\partial x_3}
        \end{aligned}&&
    \end{flalign*}
\end{frame}


\begin{frame}
    \frametitle{Multi-variable unconstrained optimization (2)}
    \setbeamertemplate{itemize items}[circle]
    \begin{itemize}
        \item Necessary condition
        $$\frac{\partial f(\underline{x}^*)}{\partial x_1}=
            \frac{\partial f(\underline{x}^*)}{\partial x_2}=
            \dots=
            \frac{\partial f(\underline{x}^*)}{\partial x_n}=0$$
        \begin{itemize}
            \item In vector form, $\triangledown f(\underline{x}^*=0)$.
            \item $\underline{x}^*$ at $\triangledown f(\underline{x}^*=0)$ is called stationary point.
        \end{itemize}
        \item Sufficient condition
        \begin{itemize}
            \item For a stationary point at $\underline{x}=\underline{x}^*$:
            \begin{itemize}
                \item if the Hessian matrix of $f(\underline{x})$ evaluated at $\underline{x}=\underline{x}^*$ is \alert{positive definite}, then $\underline{x}^*$ is a local minimum. 
                \item if the Hessian matrix of $f(\underline{x})$ evaluated at $\underline{x}=\underline{x}^*$ is \alert{negative definite}, then $\underline{x}^*$ is a local maximum. 
            \end{itemize}
        \end{itemize}
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Hessian Matrix}
    \setbeamertemplate{itemize items}[circle]
    \begin{itemize}
        \item $(H_f)_{i,j}=\frac{\partial^2f}{\partial x_i \partial x_n}$
        \item In matrix form:     $H_f=\begin{bmatrix}\frac{\partial^2f}{\partial x^2_1} & \frac{\partial^2f}{\partial x_1 \partial x_2}  & \dots & \frac{\partial^2f}{\partial x_1 \partial x_n}\\
            \frac{\partial^2f}{\partial x_2 \partial x_1} & \frac{\partial^2f}{\partial x^2_2}  & \dots & \frac{\partial^2f}{\partial x_2 \partial x_n} \\
            \vdots & \vdots & \ddots & \vdots \\
            \frac{\partial^2f}{\partial x_n \partial x_1} & \frac{\partial^2f}{\partial x_n \partial x_2}  & \dots & \frac{\partial^2f}{\partial x^2_n}
        \end{bmatrix}$
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Definiteness}
    \setbeamertemplate{itemize items}[circle]
    \begin{itemize}
        \item A matrix is positive definite if all its \alert{eigenvalues} are positive.
        \begin{itemize}
            \item If some of the eigenvalues are positive and some are zero, then the matrix is positive semidefinite.
        \end{itemize}
        \item \alert{Checking the sign of the determinants} is an alternative way to determine the definiteness of a matrix.
        \item A twice differentiable function is convex if and only if its Hessian matrix is positive
        semi-definite.
        \begin{itemize}
            \item The function is strictly convex if the Hessian matrix is positive definite.
        \end{itemize}
    \end{itemize}

\end{frame}
\begin{frame}
    \frametitle{Multivariable optimization with equality constraints}
    \textbf{Lagrange Multiplier Theorem}
    \setbeamertemplate{itemize items}[circle]
    \begin{itemize}
        \item Suppose the point $\underline{x}^*$ minimizes $f(\underline{x})$ and satisfies the equality constraints: 
        $h_j(\underline{x}^*)=0$, for $j=1,2,\dots,m$
        \item Assume that the constraint gradients $\triangledown h_j(\underline{x}^*)$ are linealy independent.
        \item Then there exists a unique set $\lambda_j^*$ ($j=1,2,\dots,m$) satisfying:
        $$\frac{\partial L}{\partial x_i}=\frac{\partial f}{\partial x_i}+\sum^m_{j=1}\lambda_j^*\frac{\partial h_j}{\partial x_i}=0$$
        where $i=1,2,\dots,n$.
    \end{itemize}

\end{frame}

\end{document}